---
title: "GFW_API"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

[Global Fishing Watch API](https://github.com/GlobalFishingWatch/gfwr)

```{r}
library(gfwr)
library(here)
library(terra)
library(raster)
library(tidyverse)
library(foreach)
library(doParallel)
library(tictoc)
library(readr)
library(dplyr)
#library(doSNOW)
library(sf)

# Save API token information to an object every time you need to extract the token and pass it to `gfwr` functions:
key <- gfw_auth()

source('http://ohi-science.org/ohiprep_v2022/workflow/R/common.R')

options(scipen = 999)

# raster template
#r_template <- raster::raster(ncol=720, nrow=360, vals=c(1:259200))
```

# Quick run through of methods that will be applied to every country and year
## Use a combination of the Vessels & Events APIs to extract fishing events from all trawlers between the dates of interest

```{r}
# extract unique trawler ID's for fishing vessels
# when we eventually execute this for the final script, we might need/want to edit the next function to loop through this function for each country, so subsetting the query for example "flag = 'USA' AND geartype = 'trawlers'"
# trawlers <- get_vessel_info(query = "geartype = 'trawlers'",
#                             search_type = "advanced",
#                             dataset = "fishing_vessel") # omit carrier vessels and support vessels

# combine all unique trawler ID's into a comma separated list that we will use for the next query
# trawler_ids <- paste0(trawlers$id[1:100], collapse = ",")
# # change this to unique(trawler$id)?
# # query all fishing events for just trawlers 
# fishing_events_trawler <- get_event(event_type = "fishing",
#                                     vessel = trawler_ids,
#                                     include_region = TRUE,
#                                     start_date = "2012-01-01",
#                                     end_date = "2013-01-01",
#                                     key = key)

# seems that we NEED to subset the list of trawler$id's in order for the next function get_event() to work. consider opening issue about that if it is not fixed?
# subsetting 1:100 worked but 1:1000 took ages
```

```{r}
# check that trawlers$id values are the vessel ID's and not the fishing event ID's
#id_check <- trawlers %>% 
#  filter(id == "a6be75fec-cab1-83bc-6e10-17687e4814e1") # that is the case!
```

## get fishing hours from diff in time stamps

```{r}
# wrangle data for total fishing hours per vessel
# fishing_events_trawler <- fishing_events_trawler %>%
#   # calculate time for each fishing event
#   mutate(fishing_event_time = end - start) %>% 
#   # change name of column to be able to have unique colnames in next step when we unnest th elist column with the vessel ID
#   rename(fishing_event_id = id) %>%
#   # unlist column that contains the vessel id so we can sum total hours by vessel
#   unnest_wider(vessel) %>% 
#   select("fishing_event_id", lat, lon, vessel_id = id, fishing_event_time) %>% 
#   group_by(vessel_id) %>% 
#   # sum total fishing hours by vessel!
#   summarize(total_vessel_fishing_time = sum(fishing_event_time))

```

## Start over process to repeat these steps for each country individually by year, then create a dataframe of all total sums with column names (rgn_id, year, total_fishing_hours)

```{r}
# load all OHI regions df:
# region_data()
# 
# #rgn = # convert OHI region to GFW 3-letter region code
#   
# #foreach(r = rgns_all$rgn_id)
# 
# trawlers <- get_vessel_info(query = paste0("flag = ", rgn, " AND geartype = 'trawlers'"),
#                             search_type = "advanced",
#                             dataset = "fishing_vessel") # omit carrier vessels and support vessels
# 
# # combine all unique trawler ID's into a comma separated list that we will use for the next query
# trawler_ids <- paste0(trawlers$id[1:100], collapse = ",")
# 
# year <- 2012:2020
# 
# # query all fishing events for just trawlers 
# fishing_events_trawler <- get_event(event_type = "fishing",
#                                     vessel = trawler_ids,
#                                     include_region = TRUE,
#                                     start_date = paste0(year, "-01-01"),
#                                     end_date = paste(year, "-12-31"),
#                                     key = key)
```

### scratch code:

```{r}
# usa_fishing <- get_vessel_info(query = "flag = 'USA'",
#                                search_type = "advanced",
#                                dataset = "fishing_vessel")
# 
# usa_ids <- paste0(usa_fishing$id[100], collapse = ',')
# usa_ids
# 
# usa_fishing$id
# 
# fishing_events_trawler <- get_event(event_type = "fishing",
#                                     vessel = usa_ids,
#                                     include_region = TRUE,
#                                     start_date = "2017-01-01",
#                                     end_date = "2017-02-01",
#                                     key = key)
# 
# fishing_events_trawler$time_diff <- fishing_events_trawler$end - fishing_events_trawler$start
```

## Use the Map Visualization API to pull in apparent fishing effort in EEZ's

```{r}
# load all OHI regions df:
# region_data()

# try one country's eez first using the ISO3 code, with only 1 eez:
# code_eez <- get_region_id(region_name = 'FJI', region_source = 'eez', key = key)
# 
# fishing_hours_16_17 <- gfwr::get_raster(spatial_resolution = 'low',
#                  temporal_resolution = 'yearly',
#                  group_by = 'flag',
#                  date_range = '2016-01-01,2017-12-31',
#                  region = code_eez$id,
#                  region_source = 'eez',
#                  key = key) %>% 
#   rename(year = "Time Range",
#          fishing_rgn = flag)


# this df represents all countries' fishing hours in that EEZ, but rows separate the lat and lon, so group by country to get the total per country by year
# in order to expand out to all eez's, i wonder if i can make a list with c() for the region argument
```

```{r}
# try with USA:
# code_eez <- get_region_id(region_name = 'USA', region_source = 'eez', key = key)
# 
# # aus_eez_ids <- paste0(code_eez$id, collapse = ",")
# 
# fishing_hours_15_19 <- gfwr::get_raster(spatial_resolution = 'low',
#                  temporal_resolution = 'yearly',
#                  group_by = 'flag',
#                  date_range = '2015-01-01,2019-12-31',
#                  region = code_eez$id[1],
#                  region_source = 'eez',
#                  key = key)

```

## Loop development: smaller subset of countries

```{r}
# region_data()
# 
# # test on subset of countries: choose large countries for which we know there is at least some fishing data
# rgns_eez_subset <- rgns_eez %>%
#   filter(eez_iso3 %in% c("USA", "CHN", "THA", "RUS"))
# 
# # add parallelization later:
# # iterate through all EEZ codes (e, outer loop) for all regions (r, inner loop) to extract apparent fishing hours:
# foreach(r = rgns_eez_subset$eez_iso3) %do% {
#   code_eez <- get_region_id(region_name = r, region_source = 'eez', key = key)
#   print(paste0("Processing apparent fishing hours for ", r, " EEZ ", code_eez$id))
#   foreach(e = code_eez$id, .combine = rbind) %do% { # .combine argument appends all eez's for that country into 1 dataframe
#     fishing_hours_15_18 <- gfwr::get_raster(spatial_resolution = 'high', # high = 0.01 degree resolution
#                                             temporal_resolution = 'yearly',
#                                             group_by = 'flagAndGearType', # since flag is the country that fished,
#                                             date_range = '2015-01-01,2018-12-31', # expand this 2012-2020
#                                             region = e,
#                                             region_source = 'eez',
#                                             key = key) %>%
#       # rename columns for clarity:
#       rename(year = "Time Range",
#              apparent_fishing_hours = "Apparent Fishing hours",
#              lat = Lat,
#              lon = Lon,
#              geartype = Geartype) %>%
#       # keep track of the administrative country for each EEZ, even after we combine all data into one dataframe:
#       mutate(eez_admin_rgn = r) %>%
#       select(year, apparent_fishing_hours, lat, lon, eez_admin_rgn, geartype) # note: we do not care about which region did the actual fishing, just about which country controls that EEZ, which is why we do not maintain the fishing region in the data moving forward
#     # convert admin region variable to factor to be able to join these columns in next step:
#     #fishing_hours_15_18$eez_admin_rgn <- as.factor(fishing_hours_15_18$eez_admin_rgn)
#     print(paste0("Extracted all apparent fishing hours for ", r, " EEZ ", e))
#     write_csv(fishing_hours_15_18, paste0(dir_M, "/git-annex/globalprep/_raw_data/global_fishing_watch/d2022/annual_mapping_api/", r, "_effort_15_18.csv"))
#   }
# }
# 
# # check out the output:
# usa <- read.csv(paste0(dir_M, "/git-annex/globalprep/_raw_data/global_fishing_watch/d2022/annual_mapping_api/USA_effort_15_18.csv"))
# chn <- read.csv(paste0(dir_M, "/git-annex/globalprep/_raw_data/global_fishing_watch/d2022/annual_mapping_api/CHN_effort_15_18.csv"))
# tha <- read.csv(paste0(dir_M, "/git-annex/globalprep/_raw_data/global_fishing_watch/d2022/annual_mapping_api/THA_effort_15_18.csv"))
# rus <- read.csv(paste0(dir_M, "/git-annex/globalprep/_raw_data/global_fishing_watch/d2022/annual_mapping_api/RUS_effort_15_18.csv"))

# consider removing any rows with NA - this might be done later anyway with na.rm when we do summary stats but also might be cleaner & faster looping to remove those rows now?
# might be easier to double check work (hours total) with other analyses if keep those rows in now?
```

## Try just Austrailia and china:

```{r}
# AUS
# aus_code_eez <- get_region_id(region_name = "AUS", region_source = 'eez', key = key)
# aus_code_eez <- as.data.frame(aus_code_eez)
# #eez_1 <- aus_code_eez[1,]
# 
# aus_fe_2012_2020_eez1 <- gfwr::get_raster(spatial_resolution = 'high', # high = 0.01 degree resolution
#                                      temporal_resolution = 'yearly',
#                                      group_by = 'flagAndGearType', # do not need to group by flag because that would be the country that fished, which we don't care about
#                                      date_range = '2012-01-01,2020-12-31', 
#                                      region = aus_code_eez$id[1], 
#                                      region_source = 'eez',
#                                      key = key)
# 
# # bind all AUS EEZ outputs into one dataframe
# # first create list of all dataframes that we want to merge
# aus_df_list <- list(aus_fe_2012_2020_eez1, aus_fe_2012_2020_eez2, aus_fe_2012_2020_eez3, aus_fe_2012_2020_eez4, aus_fe_2012_2020_eez5, aus_fe_2012_2020_eez6, aus_fe_2012_2020_eez7)
# # join
# aus_fe_allgear <- aus_df_list %>% reduce(full_join)
# 
# 
# 
# 
# # CHN
# 
chn_code_eez <- get_region_id(region_name = "CHN", region_source = 'eez', key = key)
# 
chn_2012_2020_eez1 <- gfwr::get_raster(spatial_resolution = 'high',
                                       temporal_resolution = 'yearly',
                                       group_by = 'flagAndGearType',
                                       date_range = '2020-01-01,2020-12-31',
                                       region = chn_code_eez$id[1],
                                       region_source = 'eez',
                                       key = key)

```


## Loop that is not subset: produces 1 csv per country of fishing effort for all years:

```{r}
# tic()
# 
# # load all rgn ISO codes
# region_data()
# 
# # convert regional codes into characters first:
# rgns_eez_all <- unique(rgns_eez$eez_iso3)
# 
# cl <- 3
# registerDoParallel(cl)
# 
# # iterate through all EEZ codes (e, outer loop) for all regions (r, inner loop) to extract apparent fishing hours:
# foreach(r = rgns_eez_all) %dopar% {
#   code_eez <- get_region_id(region_name = r, region_source = 'eez', key = key)
#   print(paste0("Processing apparent fishing hours for ", r, " EEZ ", code_eez$id))
#   foreach(e = code_eez$id, .combine = rbind) %do% { # .combine argument appends all eez's for that country into 1 dataframe
#     fishing_hours_2012_2020 <- gfwr::get_raster(spatial_resolution = 'high', # high = 0.01 degree resolution
#                                                 temporal_resolution = 'yearly',
#                                                 group_by = 'flagAndGearType', # do not need to group by flag because that would be the country that fished, which we don't care about
#                                                 date_range = '2012-01-01,2020-12-31', 
#                                                 region = e, 
#                                                 region_source = 'eez',
#                                                 key = key) %>%
#       # rename columns for clarity:
#       rename(year = "Time Range",
#              apparent_fishing_hours = "Apparent Fishing hours",
#              y = Lat,
#              x = Lon,
#              geartype = Geartype) %>%
#       # keep track of the administrative country for each EEZ, even after we combine all data into one dataframe: 
#       mutate(eez_admin_rgn = r) %>% 
#       select(year, apparent_fishing_hours, y, x, eez_admin_rgn, geartype) # note: we do not care about which region did the actual fishing, just about which country controls that EEZ, which is why we do not maintain the fishing region in the data moving forward
#     
#     # specify column types before saving the csv so we can correctly concatenate the rows later
#     fishing_hours_2012_2020$year <- as.numeric(fishing_hours_2012_2020$year)
#     fishing_hours_2012_2020$apparent_fishing_hours <- as.numeric(fishing_hours_2012_2020$apparent_fishing_hours)
#     fishing_hours_2012_2020$y <- as.numeric(fishing_hours_2012_2020$y)
#     fishing_hours_2012_2020$x <- as.numeric(fishing_hours_2012_2020$x)
#     fishing_hours_2012_2020$eez_admin_rgn <- as.character(fishing_hours_2012_2020$eez_admin_rgn)
#     fishing_hours_2012_2020$geartype <- as.character(fishing_hours_2012_2020$geartype)
#     
#     print(paste0("Extracted all apparent fishing hours for ", r, " EEZ ", e))
#     write_csv(fishing_hours_2012_2020, paste0(dir_M, "/git-annex/globalprep/_raw_data/global_fishing_watch/d2022/annual_mapping_api/all_regions/", r, "_effort_2012_2020.csv")) 
#   }
# }
# 
# stopCluster(cl)
```

## fix loop to concatenate rows correctly for eez codes by country 

```{r}
tic()
# load all rgn ISO codes
region_data()

# test on subset of countries: choose large countries for which we know there is at least some fishing data
rgns_subset <- rgns_eez %>%
  filter(eez_iso3 %in% c("AUS", "USA", "THA", "BIH", "GBR", "ITA"))
# removed CHN from list because cannot process one of the two EEZ: 8486 (first eez in list of 2 from API)

rgns_subset <- unique(rgns_subset$eez_iso3)

# convert regional codes into characters first:
#rgns_eez_all <- unique(rgns_eez$eez_iso3)

#cl <- 3
#registerDoParallel(cl)

# iterate through all EEZ codes (e, outer loop) for all regions (r, inner loop) to extract apparent fishing hours:
for(i in rgns_subset) {
  # create dataframe that contains the column `id` that is list of all EEZ codes for one region
  eez_code_df <- get_region_id(region_name = i, region_source = 'eez', key = key)
  # convert that column into a numeric list of EEZ codes to feed into the next loop:
  eez_codes <- eez_code_df$id
  
  print(paste0("Processing apparent fishing hours for ", i, " EEZ code ", eez_codes))
  
  for(j in eez_codes) { 
    fishing_hours_2012_2020 <- gfwr::get_raster(spatial_resolution = 'high', # high = 0.01 degree resolution which we think is close to 30 m resolution
                                                temporal_resolution = 'yearly',
                                                group_by = 'flagAndGearType', # maybe change to just geartype
                                                date_range = '2012-01-01,2020-12-31', 
                                                region = j, 
                                                region_source = 'eez',
                                                key = key) %>%
      # rename columns for clarity:
      rename(year = "Time Range",
             apparent_fishing_hours = "Apparent Fishing hours",
             y = Lat,
             x = Lon,
             geartype = Geartype) %>%
      # keep track of the administrative country for each EEZ, even after we combine all data into one dataframe: 
      mutate(eez_admin_rgn = i) %>% 
      select(year, apparent_fishing_hours, y, x, eez_admin_rgn, geartype) # note: we do not care about which region did the actual fishing, just about which country controls that EEZ, which is why we do not maintain the fishing region in the data moving forward
    
    # specify column types before saving the csv so we can correctly concatenate the rows later
    fishing_hours_2012_2020$year <- as.numeric(fishing_hours_2012_2020$year)
    fishing_hours_2012_2020$apparent_fishing_hours <- as.numeric(fishing_hours_2012_2020$apparent_fishing_hours)
    fishing_hours_2012_2020$y <- as.numeric(fishing_hours_2012_2020$y)
    fishing_hours_2012_2020$x <- as.numeric(fishing_hours_2012_2020$x)
    fishing_hours_2012_2020$eez_admin_rgn <- as.character(fishing_hours_2012_2020$eez_admin_rgn)
    fishing_hours_2012_2020$geartype <- as.character(fishing_hours_2012_2020$geartype)
    
    print(paste0("Extracted all apparent fishing hours for ", i, " EEZ code ", j))
    
    write_csv(fishing_hours_2012_2020, paste0(dir_M, "/git-annex/globalprep/_raw_data/global_fishing_watch/d2022/annual_mapping_api/", i, "_", j, "_effort_2012_2020.csv")) 
  }
}

#stopCluster(cl)

toc() # 11 min
```


## Exploration: Check what some of the country-specific dataframes contains:

```{r}
# rus <- read.csv(paste0(dir_M, "/git-annex/globalprep/_raw_data/global_fishing_watch/d2022/annual_mapping_api/all_regions/RUS_effort_2012_2020.csv"))
# chn <- read.csv(paste0(dir_M, "/git-annex/globalprep/_raw_data/global_fishing_watch/d2022/annual_mapping_api/all_regions/CHN_effort_2012_2020.csv"))
# #spain:
# esp <- read.csv(paste0(dir_M, "/git-annex/globalprep/_raw_data/global_fishing_watch/d2022/annual_mapping_api/all_regions/ESP_effort_2012_2020.csv"))
# bih <- read.csv(paste0(dir_M, "/git-annex/globalprep/_raw_data/global_fishing_watch/d2022/annual_mapping_api/all_regions/BIH_effort_2012_2020.csv"))
```

## Dataset cleaning: remove files with 0 rows in order to combine all files row-wise into one object in next steps

Document which files (EEZ regions) had no fishing detected by GFW AIS data in 2012-2020 in order to notice trends over years.

```{r}
fish_effort_files <- list.files(paste0(dir_M, "/git-annex/globalprep/_raw_data/global_fishing_watch/d2022/annual_mapping_api/"), pattern = ".csv", full = TRUE)

# first check the length to see how many files we start with
num_files_start <- length(fish_effort_files)
print(paste0("Starting with ", num_files_start, " files."))

# delete files that do not have any rows (because no fishing has been recorded in that EEZ)
for (i in seq_along(fish_effort_files)) {
  # read each file and check number of rows
  filename <- fish_effort_files[i]
  print(paste0("Counting rows (fishing effort observations) for ", substr(filename, -29, -21)))
  # save the numbe of rows for that file to ab object
  rows <- nrow(data.table::fread(filename))
  print(paste0(rows, " rows in this file."))
  # if there are 0 rows, delete the file
  if (rows == "0") {
      unlink(filename) 
    }
}

# redefine variable after deleting some files
fish_effort_files <- list.files(paste0(dir_M, "/git-annex/globalprep/_raw_data/global_fishing_watch/d2022/annual_mapping_api/"), pattern = ".csv", full = TRUE)

# check length again to see how many files we end with
  num_files_end <- length(fish_effort_files)
  print(paste0("Ending with ", num_files_end, " files because ", (num_files_start - num_files_end), " files were deleted for containing no data."))
# 2022 assessment: __ files were removed because had 0 rows, indicating no fishing activity at all within those countries' EEZs in 2012-2020 (from GFW AIS detections):
```

## Data Wrangling: Combine regional csv's into one dataframe and filter by geartype

We do this because our goal is to summarize fishing effort spatially to produce annual rasters of fishing effort. Later, we will use the fishing coordinates and EEZ boundaries to attribute fishing activity to certain OHI regions.

```{r}
# concatenate csv's into one list for trawling data, separate from dredging so we can correct the trawling data and then sum the corrected trawling data with the dredging data 
fish_effort_trawl <- fish_effort_files %>%
  lapply(data.table::fread) %>% # read in each file that represents data for 1 EEZ for 1 country
  bind_rows() %>% # combine all files into one dataframe
  filter(geartype == "trawlers") %>% # trawl fishing damages the seafloor
  dplyr::select(-geartype) # now that all observations are trawlers, we can drop this variable

# concatenate csv's into one list for dredging data, we will later sum this with the corrected trawling data
fish_effort_dredge <- fish_effort_files %>%
  lapply(data.table::fread) %>% # read in each file that represents data for 1 EEZ for 1 country
  bind_rows() %>% # combine all files into one dataframe
  filter(geartype == "dredge_fishing") %>% # dredge fishing damages the seafloor
  dplyr::select(-geartype) # now that all observations are from dredging, we can drop this variable

# take a look at the data
head(fish_effort_dredge)
```

## Group by coordinate and year for apparent fishing effort for both geartypes of interest

We do not group by the administrative region for each EEZ because the coordinates of the fishing activity will allow us to attribute fishing activity to OHI regions. 

```{r}
# trawling
fish_effort_trawl_annual <- fish_effort_trawl %>% # change to the relevant trawl data after filtering for that with catch data later
  group_by(x, y, year) %>% # 2022 assessment: did not group by year (according to output) because no exact coordinate was repeated one multiple years, so we will do that in next step
  summarize(total_fishing_hours = sum(apparent_fishing_hours, na.rm = TRUE)) 
# convert year column from integer to factor in preparation for next steps:
fish_effort_trawl_annual$year <- as.factor(fish_effort_trawl_annual$year)

# dredging
fish_effort_dredge_annual <- fish_effort_dredge %>% # all dredge data will be used, we will not be suubsetting it like we will for the trawling data 
  group_by(x, y, year) %>% # 2022 assessment: did not group by year (according to output) because no exact coordinate was repeated one multiple years, so we will do that in next step
  summarize(total_fishing_hours = sum(apparent_fishing_hours, na.rm = TRUE)) 
# convert year column from integer to factor in preparation for next steps:
fish_effort_dredge_annual$year <- as.factor(fish_effort_dredge_annual$year)

# take a look
head(fish_effort_dredge_annual)
```

## Check one annual raster of trawler fishing effort: 2015 as a test before doing all years in loop

```{r}
# try with just 2015 trawling
# rasterize the data:

fish_effort_trawl_2015 <- fish_effort_trawl_annual %>%
  dplyr::filter(year == "2015") %>%
  dplyr::select(-year)

# convert the dataframe to a SpatRaster
# sometimes running this for the first time fails bur just run again and it works
fish_effort_trawl_2015_rast <- terra::rast(fish_effort_trawl_2015, type = "xyz", crs = "EPSG:4326", digits = 6, extent = NULL)

plot(fish_effort_trawl_2015_rast, col = "red") 

# convert to points vector (a geometry column?)
#fish_effort_2015_points <- terra::as.points(fish_effort_2015_rast)
#plot(fish_effort_2015_points, col = "red") # looks like blobs where the fishing points were! I suppose the difference between coordinates and points is that points just appear larger when plotted

# load the EEZ spatial data to layer on plot with fishing effort points for reference
regions_shape()

# check out df subset to not overwhelm Mazu by opening regions df in its entirety
View(head(regions))

# filter the regions dataframe for just EEZ polygons
regions_eez <- regions %>%
  filter(rgn_type == "eez") %>% 
  st_transform(crs = 4326)

# try plotting just geometry
plot(regions_eez$geometry, col = "red")

# try to plot points on polygons
#plot(regions_eez$geometry, col = "grey96", axes = FALSE, main = "fishing effort 2015", legend = FALSE)		
#plot(fish_effort_2015_points, axes = FALSE, col = "red", add = TRUE) # points are large! not great for viz

# Plot the raster of fishing effort points (without making them terra points) on top of the EEZ polygons
plot(regions_eez$geometry, col = "grey96", axes = FALSE, main = "Fishing Effort 2015: AUS, USA, THA, BIH, GBR, ITA", legend = FALSE)
plot(fish_effort_trawl_2015_rast, axes = FALSE, col = "red", add = TRUE)
```

### Save the trawling and dredging data as annual raster files before we can adjust them for mid-water versus bottom trawling.

- would like to add checks to this, progress bar, more cores for parallelization

```{r}
tic()

years = as.factor(2012:2020) # make it factor in order to match the class of year column in the fish_effort_annual dataframe

for(i in years){
  
  # trawling data:
  trawl_annual_raster <- fish_effort_trawl_annual %>%
    dplyr::filter(year == i) %>%
    dplyr::select(-year) %>%
    terra::rast(type = "xyz", crs = "EPSG:4326", digits = 6, extent = NULL)
  
  # save annual raster file for trawling:
  terra::writeRaster(trawl_annual_raster, filename = paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/trawling/", i,"/fish_effort_trawl_", i, ".tif"), overwrite = TRUE)
  
  # dredging data:
  dredge_annual_raster <- fish_effort_dredge_annual %>%
    dplyr::filter(year == i) %>%
    dplyr::select(-year) %>%
    terra::rast(type = "xyz", crs = "EPSG:4326", digits = 6, extent = NULL)
  
  # save annual raster file for dredging:
  terra::writeRaster(dredge_annual_raster, filename = paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/dredging/fish_effort_dredge_", i , ".tif"), overwrite = TRUE)

}
# note that the trawling data is separated into annual folders within the trawling folder, and the dredging data is not separated into annual folders. This is because we will be subsetting the trawling data for just bottom trawling later by making a spatRaster stack of the trawling and catch data matched by year, and its easier to create those stacked objects using terra if the matching annual files are within the same directory
toc()
```

### Correct Trawling Data: Subset trawling fishing effort by catch data for the corresponding year to distinguish between mid-water trawling and bottom trawling

We use fisheries catch data from the paper by Watson et al. to distinguish between the two types of trawling, because the GFW data groups all trawling together. We only maintain bottom trawling because that is the type that destroys soft bottom habitat. We multiply a raster that represents a proxy for the proportion of mid-water trawling to bottom trawling by the Global Fishing Watch raster for that respective year. We then sum that data with all the dredging data in the next steps. We only have catch data (the trawling correction data) for 2012-2017, so we need to use 2017 data for 2018-2020 GFW data.

```{r}
# fish_effort_files <- list.files(paste0(dir_M, "/git-annex/globalprep/_raw_data/global_fishing_watch/d2022/annual_mapping_api/"), pattern = ".csv", full = TRUE)
# first read in the annual rasters that represent the proportion of mid-water trawling versus bottom trawling, cell values range 0-1
# these files were created on July 29, 2022 from Watson et al. data by Gage
# value of 0 = all mid-water trawling
# value of 1 = all bottom trawling
trawl_depth_proportion_2012 <- terra::rast(paste0(dir_M, '/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/trawling_correction/bottom_trawl_props/bottom_trawl_prop_2012.tif'))
# plot 2012 data to get an idea of what it looks like
# pretty interesting that the EEZ outlines can be distinguished in the trawling locality
terra::plot(trawl_depth_proportion_2012)

# check how many NA values we start with
value <- terra::global(trawl_depth_proportion_2012, fun = "isNA") # 191,377
value[1,1]
terra::global(trawl_depth_proportion_2012, fun = "isNA")[1,1]
# read in years 2013-2017
# trawl_depth_proportion_2013 <- terra::rast(paste0(dir_M, '/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/trawling_correction/bottom_trawl_props/bottom_trawl_prop_2013.tif'))
# trawl_depth_proportion_2014 <- terra::rast(paste0(dir_M, '/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/trawling_correction/bottom_trawl_props/bottom_trawl_prop_2014.tif'))
# trawl_depth_proportion_2015 <- terra::rast(paste0(dir_M, '/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/trawling_correction/bottom_trawl_props/bottom_trawl_prop_2015.tif'))
# trawl_depth_proportion_2016 <- terra::rast(paste0(dir_M, '/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/trawling_correction/bottom_trawl_props/bottom_trawl_prop_2016.tif'))
# trawl_depth_proportion_2017 <- terra::rast(paste0(dir_M, '/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/trawling_correction/bottom_trawl_props/bottom_trawl_prop_2017.tif'))

# make this into a loop when figured it out for 2012
# check the number of na values in that raster
#terra::global(trawl_depth_proportion_2012, fun = "isNA") # 191377	to start 
# check the number of not na values
#terra::global(trawl_depth_proportion_2012, fun = "notNA") # 67823 to start

# try to take the mean gage did, with raster, to see what output is supposed to look like 
# trawl_depth_proportion_2012_raster <- raster::raster(paste0(dir_M, '/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/trawling_correction/bottom_trawl_props/bottom_trawl_prop_2012.tif'))
# avg_trawl_correction_raster <- cellStats(trawl_depth_proportion_2012_raster, "mean", na.rm=TRUE)
# plot(avg_trawl_correction_raster) # 1 value not an actual raster 

#terra::app() is not what we want
#terra::mean() is not what we want 
# maybe global is what we want?
# maybe terra::approximate() is what we want?
#trawl_depth_proportion_2012_interpolated <- terra::global(trawl_depth_proportion_2012, fun = "mean")
# maybe terra::focal is what we want ? best option so far, would be even more accurate than taking mean across entire layer because takes a local avg with window
trawl_depth_proportion_interpolated_2012 <- terra::focal(x = trawl_depth_proportion_2012, 
                                                         w = 9, 
                                                         fun = "mean", 
                                                         na.policy = "only", 
                                                         na.rm = T)

# check how many NA values were interpolated
terra::global(trawl_depth_proportion_interpolated_2012, fun = "isNA") # 102,832

# na.only = "only" argument means we interpolate for only the NA values of the raster 
# w = 9 means the moving window is 9 cells by 9 cells (a square) around the focal point we are interpolating
# code help from: https://stackoverflow.com/questions/71801889/how-to-fill-in-missing-na-values-in-raster-with-terra-package
terra::plot(trawl_depth_proportion_interpolated_2012)
# check the number of na values in that raster after interpolation
#terra::global(trawl_depth_proportion_2012_interpolated, fun = "isNA") # 102832, the number of na values decreased by 88545
# check the number of not na values
#terra::global(trawl_depth_proportion_2012_interpolated, fun = "notNA") # 156368, the number of not na values increased by 88454

# now that the trawl proportion data has been interpolated, we need to adjust its spatial resolution to match that of the fishing effort data from GFW for the corresponding year 
# first read in the GFW raster for the year of interest
fish_effort_trawl_2012 <- terra::rast(paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/trawling/2012/fish_effort_trawl_2012.tif"))
# resample the trawling proportion data to match the resolution and extent of the GFW fishing effort data (use the GFW as the sample geometry)
trawl_depth_proportion_resampled_2012 <- terra::resample(x = trawl_depth_proportion_interpolated_2012, y = fish_effort_trawl_2012, method = "near", filename = paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/trawling/2012/trawl_depth_proportion_resampled_2012.tif"), overwrite = TRUE)
# question: should we resample before we interpolate perhaps? - note to ask Gage & Mel
terra::plot(trawl_depth_proportion_resampled_2012)

# check extents of each - they match! 
ext(terra::rast(paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/trawling/2012/fish_effort_trawl_2012.tif")))
ext(terra::rast(paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/trawling/2012/trawl_depth_proportion_resampled_2012.tif")))

# stack the resampled interpolated raster of bottom trawling with all trawling data from gfw, matching the data by year for 2012-2017 and then using 2017 for the rest of the years of gfw trawling data for which we do not have watson data 
trawl_stack_2012 <- terra::rast(list.files(paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/trawling/2012"), pattern = ".tif", full = TRUE))

# multiply the rasters in order to only maintain the proportion of trawling fishing effort that is attributed to bottom trawling rather than midwater trawling
trawling_corrected_2012 <- terra::lapp(trawl_stack_2012, fun = function(x,y){return(x*y)}, filename = paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/trawling_correction/trawling_corrected/trawling_corrected_2012.tif"), overwrite = TRUE)

terra::plot(trawling_corrected_2012, col = "red")
```


```{r}
tic()

# use this subset because we only have trawling proportion data for these years
years_subset = as.factor(2012:2017)
  
# for years with trawling depth proportion data:
for (j in years_subset){
  
  print(paste0("Processing trawling fishing effort for ", j))
  # read in the GFW apparent fishing effort raster for trawling, we need this read in to resample the Watson catch data to the same resolution
  fish_effort_trawl <- terra::rast(paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/trawling/", j, "/fish_effort_trawl_", j, ".tif"))
  # read in the annual file that represents the proportion of bottom trawling to midwater trawling, based on catch data, from Watson 
  trawl_depth_proportion <- terra::rast(paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/trawling_correction/bottom_trawl_props/bottom_trawl_prop_", j, ".tif"))
  
  print(paste0("Interpolating trawling depth proportion data for ", j))
  # interpolate many of the missing values in the trawling depth proportion raster by taking the local average of cells around it (using a window of 9 cells) that represent the proportion of bottom trawling to midwater trawling that occurred at that coordinate. This provides a more comprehensive raster which we will later multiply by the GFW trawling raster, cell by cell
  trawl_depth_proportion_interpolated <- terra::focal(x = trawl_depth_proportion, 
                                                      w = 5, 
                                                      fun = "mean", 
                                                      na.policy = "only", 
                                                      na.rm = T,
                                                      overwrite = TRUE)
  
  # document how many NA values were filled by `focal`
  print(paste0("Started with ", terra::global(trawl_depth_proportion, fun = "isNA")[1,1], " NA values, and ended with ", terra::global(trawl_depth_proportion_interpolated, fun = "isNA")[1,1], " so filled ", terra::global(trawl_depth_proportion, fun = "isNA")[1,1] - terra::global(trawl_depth_proportion_interpolated, fun = "isNA")[1,1], " NA values using `terra::focal()`."))
  
  # clear some memory
  rm(trawl_depth_proportion)
  gc()
  
  # fill in remaining NA values with the global mean of the non-NA values
  global_trawl_proportion_avg <- terra::global(trawl_depth_proportion_interpolated, fun = "mean", na.rm = TRUE)
  trawl_depth_proportion_interpolated[is.na(trawl_depth_proportion_interpolated)] <- global_trawl_proportion_avg[1,1]
  
  # document how many NA values remain after applying the local average
  print(paste0("There are now ", terra::global(trawl_depth_proportion_interpolated, fun = "isNA"), " NA values present after filling the remaining Na's with the global average."))
  
  print(paste0("Resampling trawling depth proportion data for ", j))
  
  # resample the interpolated trawling proportion data to match the resolution of the GFW fishing effort data
  trawl_depth_proportion_resampled <- terra::resample(x = trawl_depth_proportion_interpolated, 
                                                      y = fish_effort_trawl, # use the GFW data as the sample geometry
                                                      method = "near") # nearest neighbor
  
  # document how many NA values were filled 
  print(paste0("Now there are ", terra::global(trawl_depth_proportion_resampled, fun = "isNA")[1,1], " NA values remaining."))
  
  # save the adjusted bottom trawling proportion raster in the folder of the corresponding year of GFW data, & save the most recent year of proportion data (2017) for 2017-2020 GFW data
  if (j == 2017) {
    # save to folders for 2017-2020
    print(paste0("Saving ", j, " file to ", j, " folder."))
    terra::writeRaster(x = trawl_depth_proportion_resampled, filename = paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/trawling/", j, "/trawl_depth_proportion_resampled_", j, ".tif"), overwrite = TRUE)
    print(paste0("Saving ", j, " file to 2018 folder."))
    terra::writeRaster(x = trawl_depth_proportion_resampled, filename = paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/trawling/2018/trawl_depth_proportion_resampled_", j, ".tif"), overwrite = TRUE)
    print(paste0("Saving ", j, " file to 2019 folder."))
    terra::writeRaster(x = trawl_depth_proportion_resampled, filename = paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/trawling/2019/trawl_depth_proportion_resampled_", j, ".tif"), overwrite = TRUE)
    print(paste0("Saving ", j, " file to 2020 folder."))
    terra::writeRaster(x = trawl_depth_proportion_resampled, filename = paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/trawling/2020/trawl_depth_proportion_resampled_", j, ".tif"), overwrite = TRUE)
  } else {
    # for trawling proportion data for years 2012-2016, save file to just that year's folder
    print(paste0("Saving ", j, " file to ", j, " folder."))
    terra::writeRaster(x = trawl_depth_proportion_resampled, filename = paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/trawling/", j, "/trawl_depth_proportion_resampled_", j, ".tif"), overwrite = TRUE)
  }  
}

  
# clear some memory
rm(trawl_depth_proportion_resampled)
gc()

toc()
```

```{r}
# scratch code
# check the origin and resolution of the rasters btw data sources
# read in the GFW apparent fishing effort raster for trawling, we need this read in to resample the Watson catch data to the same resolution
fish_effort_trawl <- terra::rast(paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/trawling/2012/fish_effort_trawl_2012.tif"))
# read in the annual file that represents the proportion of bottom trawling to midwater trawling, based on catch data, from Watson 
trawl_depth_proportion <- terra::rast(paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/trawling_correction/bottom_trawl_props/bottom_trawl_prop_2012.tif"))

fish_effort_trawl # res 0.01, 0.01, ext -179.985, 180.005, -55.305, 73.065  (xmin, xmax, ymin, ymax)
trawl_depth_proportion # res 0.5, 0.5, ext -180, 180, -90, 90  (xmin, xmax, ymin, ymax)

origin(fish_effort_trawl)
origin(trawl_depth_proportion)

terra::global(fish_effort_trawl, fun = "isNA")

```


```{r}
# 2017
# test that raster that was saved looks like we would expect compared to unprocessed raster from Watson
watson_unprocessed_2017 <- terra::rast(paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/trawling_correction/bottom_trawl_props/bottom_trawl_prop_2017.tif"))

plot(watson_unprocessed_2017)

terra::global(watson_unprocessed_2017, fun = "isNA") # 201,000

watson_processed_2017 <- terra::rast(paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/trawling/2017/trawl_depth_proportion_resampled_2017.tif"))

plot(watson_processed_2017)

terra::global(watson_processed_2017, fun = "isNA") # 156,880,905

# 2012
# test that raster that was saved looks like we would expect compared to unprocessed raster from Watson
watson_unprocessed_2012 <- terra::rast(paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/trawling_correction/bottom_trawl_props/bottom_trawl_prop_2012.tif"))

plot(watson_unprocessed_2012)

terra::global(watson_unprocessed_2012, fun = "isNA") # 191377

watson_processed_2012 <- terra::rast(paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/trawling/2012/trawl_depth_proportion_resampled_2012.tif"))

plot(watson_processed_2012)

terra::global(watson_processed_2012, fun = "isNA") # 135,657,275			










# check extents of the rasters in this folder 2018
rasters <- list.files(paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/trawling/2018"), pattern = ".tif", full = TRUE)
fish_effort_raster <- terra::rast(rasters[1])
trawl_depth_proportion_raster <- terra::rast(rasters[2])
extent_comparison <- terra::compareGeom(fish_effort_raster, trawl_depth_proportion_raster, stopOnError = FALSE)
print(paste0("Raster extent comparison results: ", extent_comparison))

if (extent_comparison == FALSE){ 
    print("Cropping extent of trawling proportion raster to match the extent of the trawling fishing effort raster.")
  
    trawl_depth_proportion_cropped <- terra::crop(trawl_depth_proportion_raster, fish_effort_raster)
    
    # check the extents now
    ext(fish_effort_raster)
    ext(trawl_depth_proportion_cropped)
  }




```

#### Apply the adjusted (resampled and interpolated) Watson rasters to correct the GFW trawling data

```{r}
tic()

years_all = as.factor(2012:2020)
years_test = as.factor(2017:2018)

for (i in years_test){
  # read in the GFW apparent fishing effort raster for trawling, we need this read in because we will multiple GFW rasters with the corresponding Watson catch data rasters 
  #print(paste0("Reading trawling fishing effort for ", i))
  #fish_effort_trawl <- terra::rast(paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/trawling/", i, "/fish_effort_trawl_", i, ".tif"))
  
  # stack the resampled interpolated raster of bottom trawling with all trawling data for that year from GFW, matching the data by year for 2012-2017 and then using 2017 for the rest of the years of GFW trawling data for which we do not have more recent trawling proportion data
  print(paste0("Stacking trawling data for ", i))
  # first read in both rasters within the annual folders and save the list of 2 as an object 
  rasters <- list.files(paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/trawling/", i), pattern = ".tif", full = TRUE)
  
  # read in the rasters individually in order to check if extents match
  fish_effort_raster <- terra::rast(rasters[1])
  trawl_depth_proportion_raster <- terra::rast(rasters[2])
  # check if the extents match
  extent_comparison <- terra::compareGeom(fish_effort_raster, trawl_depth_proportion_raster, stopOnError = FALSE)
  print(paste("Raster extent comparison results: ", extent_comparison))
  
  # if the extents do not match, crop the trawl depth proportion raster to match the extent of the fishing effort raster from GFW
  if (extent_comparison == FALSE){ 
    print(paste0("Cropping extent of trawling proportion raster to match the extent of the trawling fishing effort raster."))
    trawl_depth_proportion_cropped <- terra::crop(trawl_depth_proportion_raster, fish_effort_raster)
    
    # re-write the cropped raster to that folder, with a different name to keep track of which rasters were cropped
    terra::writeRaster(trawl_depth_proportion_cropped, paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/trawling/", i, "/trawl_depth_proportion_raster_cropped.tif"), overwrite = TRUE)
    
    print(paste0("Deleting the old trawl depth proportion raster that was not the same extent as the GFW raster."))
    # delete the old trawl depth proportion raster that was not yet cropped to the extent of the fishing effort raster
    unlink(rasters[2])
    
    # check extents again in case it only cropped the raster in certain dimensions and needs to be further manipulated
    # first read in both rasters within the annual folders and save the list of 2 as an object 
    rasters <- list.files(paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/trawling/", i), pattern = ".tif", full = TRUE)
  
    # read in the rasters individually in order to check if extents match
    fish_effort_raster <- terra::rast(rasters[1])
    trawl_depth_proportion_raster <- terra::rast(rasters[2])
    # check if the extents match
    extent_comparison <- terra::compareGeom(fish_effort_raster, trawl_depth_proportion_raster, stopOnError = FALSE)
    print(paste("Raster extent comparison results: ", extent_comparison))
  }
  
  # create the stack of 2 rasters in the annual folder
  trawl_stack <- terra::rast(rasters)
  
  print(paste0("Correcting trawling effort data for ", i))
  
  # multiply the rasters in order to only maintain the proportion of trawling fishing effort that is attributed to bottom trawling rather than midwater trawling
  terra::lapp(trawl_stack, fun = function(x,y){return(x*y)}, filename = paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/trawling_correction/trawling_corrected/trawling_corrected_", i, ".tif"), overwrite = TRUE)
  
  # clear some memory
  rm(trawl_stack)
  gc()
  
  print(paste0("Saved corrected trawling data for ", i, " to ", dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/trawling_correction/trawling_corrected/trawling_corrected_", i, ".tif"))
}
  
toc()
```

```{r}
# scratch code 
# check extents of 2017 trawl prop data with 2018-2020
rasters <- list.files(paste0(dir_M, "/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/trawling/2012"), pattern = ".tif", full = TRUE)
raster_1 <- terra::rast(rasters[1])
raster_2 <- terra::rast(rasters[2])
result <- terra::compareGeom(raster_1, raster_2)
result
terra::ext(raster_1)
terra::ext(raster_2)
# trawl depth proportion is larger
# extend GFW raster to trawl depth proportion raster extent
raster_3 <- terra::extend(raster_1, raster_2)
raster_4 <- terra::extend(raster_2, raster_3)
terra::ext(raster_3)
```


















```{r}
# rasterize the dredging fishing effort dataframe - data from GFW
terra::rast(fish_effort_dredge_annual, )






## Gage's code:

#Sum the corrected trawl with the dredge effort to get the total demersal destructive fishing effort.
corrected_trawl <- raster("/home/shares/food-systems/Food_footprint/all_food_systems/dataprep/fisheries/marine/disturbance/global_fishing_watch/FishingWatch_annual_effort_destructive_trawlers_2017.tif")

dredge <- raster("/home/shares/food-systems/Food_footprint/all_food_systems/dataprep/fisheries/marine/disturbance/global_fishing_watch/FishingWatch_annual_effort_dredge_2017.tif")

newextent=c(-180.005, 179.995, -89.995, 90.005)

dredge_extend <- extend(dredge, newextent, value=0)
corrected_trawl_extend <- extend(corrected_trawl, newextent, value=0)

damage_stack <- stack(dredge_extend, corrected_trawl_extend)

calc(damage_stack, sum, na.rm=TRUE,
 filename = "/home/shares/food-systems/Food_footprint/all_food_systems/dataprep/fisheries/marine/disturbance/global_fishing_watch/FishingWatch_annual_effort_destructive_hours_2017.tif", overwrite=TRUE, progress="text")
```







































## Exploration: Check that regions that are infamous for trawling are recorded as trawling as much as we would expect:

```{r}
# usa <- fish_effort_all %>% 
#   filter(eez_admin_rgn == "USA") %>% 
#   group_by(year) %>% 
#   summarize(total_fishing_hours = sum(apparent_fishing_hours))
# 
# chn <- fish_effort_all %>% 
#   filter(eez_admin_rgn == "CHN") %>% 
#   group_by(year) %>% 
#   summarize(total_fishing_hours = sum(apparent_fishing_hours))
# 
# nzl <- fish_effort_all %>% 
#   filter(eez_admin_rgn == "NZL") %>% 
#   group_by(year) %>% 
#   summarize(total_fishing_hours = sum(apparent_fishing_hours))

# see which countries have the most trawling
# rgn_trawl <- fish_effort_all %>% 
#   group_by(eez_admin_rgn) %>% 
#   summarise(total_fishing_hours = sum(apparent_fishing_hours))
# 
# rgn_trawl_max <- rgn_trawl %>% 
#   slice_max(total_fishing_hours, n = 20, with_ties = FALSE) %>%
#   arrange(desc(total_fishing_hours)) 
# 
# # visualize distribution of all regions trawling
# trawling_all <- ggplot(data = rgn_trawl_max, aes(x = eez_admin_rgn, y = total_fishing_hours)) +
#   geom_point() +
#   geom_point(size = 3) +
#   labs(title = "Top 20 Trawling Regions, 2012-2020",
#        subtitle = "GFW Data",
#        x = "Region Code",
#        y = "Total Trawling Fishing Effort Hours") +
#   theme_minimal()
# 
# trawling_all
```

## Data Wrangling: all gear types

```{r}
# fish_effort_allgear <- list.files(paste0(dir_M, "/git-annex/globalprep/_raw_data/global_fishing_watch/d2022/annual_mapping_api/all_regions"), pattern = ".csv", full = TRUE) %>%
#   lapply(data.table::fread) %>%
#   bind_rows()
# 
# head(fish_effort_allgear)
# 
# unique(fish_effort_allgear$eez_admin_rgn)
# 
# # see which countries have the most fishing in general - all geartypes
# rgn_fe <- fish_effort_allgear %>% 
#   group_by(eez_admin_rgn) %>% 
#   summarise(total_fishing_hours = sum(apparent_fishing_hours))
# 
# rgn_fe_max <- rgn_fe %>% 
#   slice_max(total_fishing_hours, n = 20, with_ties = FALSE) %>%
#   arrange(desc(total_fishing_hours)) 
# 
# # visualize distribution of all regions trawling
# trawling_allgear <- ggplot(data = rgn_fe_max, aes(x = eez_admin_rgn, y = total_fishing_hours)) +
#   geom_point() +
#   geom_point(size = 3) +
#   labs(title = "Top 20 Fishing Regions (all geartypes), 2012-2020",
#        subtitle = "GFW Data",
#        x = "Region Code",
#        y = "Total Fishing Effort Hours") +
#   theme_minimal()
# 
# trawling_allgear
```


## Group apparent trawling fishing effort by latitude, longitude, and year for raster analysis

```{r}
# group all countries and years by lat, long, and year (this df does not have the EEZ admin country anymore, since we don't need that at the moment)
fish_effort_annual <- fish_effort_trawl %>% # change to the relevant trawl data after filtering for that with catch data later
  group_by(x, y, year) %>% # 2022 assessment: did not group by year (according to output) because no exact coordinate was repeated one multiple years, so we will do that in next step
  summarize(total_fishing_hours = sum(apparent_fishing_hours, na.rm = TRUE)) 

head(fish_effort_annual)

# convert year column from integer to factor in preparation for next steps:
fish_effort_annual$year <- as.factor(fish_effort_annual$year)

# save dataframe before rasterizing:
#write_csv(, ".csv")
```

## Check one annual raster of trawler fishing effort: 2015 as a test before doing all years in loop

```{r}
# try with just 2015 first before loop:
# rasterize the data:

fish_effort_2015 <- fish_effort_trawl %>%
  dplyr::filter(year == "2015") %>%
  dplyr::select(-year)

# set spatial coordinates of a dataframe
# sp::coordinates(fish_effort_2015) <- ~x+y # errors if use "x" + "y"
# # assign EPSG 4326 as the CRS
# raster::rasterFromXYZ(fish_effort_2015, crs = "+init=epsg:4326", digits = 6) 
# proj4string(fish_effort_2015) = CRS("+init=epsg:4326") # code from Gage's footprint project
# #raster::rasterFromXYZ(fish_effort_2015, crs = "EPSG:4326", digits = 6)
# crs(fish_effort_2015)
# plot(fish_effort_2015, col = "red") # plot in red to visualize points easier

# check if using diff raster function makes output diff?
#raster::writeRaster(fish_effort_2015, filename = paste0(dir_M, '/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/test.tif'), overwrite = TRUE)

# convert the dataframe to a SpatRaster
fish_effort_2015_rast <- terra::rast(fish_effort_2015)

plot(fish_effort_2015_rast, col = "red") # looks like points but needs to be a df to intersect with EEZ polygons

# convert to points vector (a geometry column?)
#fish_effort_2015_points <- terra::as.points(fish_effort_2015_rast)

#plot(fish_effort_2015_points, col = "red") # looks like blobs where the fishing points were! I suppose the difference between coordinates and points is that points just appear larger when plotted

# load the EEZ spatial data to layer on plot with fishing effort points for reference
regions_shape()

# check out df subset to not overwhelm Mazu by opening regions df in its entirety
View(head(regions))

# filter the regions dataframe for just EEZ polygons
regions_eez <- regions %>%
  filter(rgn_type == "eez") %>% 
  st_transform(crs = 4326)

# try plotting just geometry
plot(regions_eez$geometry, col = "red")

# try to plot points on polygons
#plot(regions_eez$geometry, col = "grey96", axes = FALSE, main = "fishing effort 2015", legend = FALSE)		
#plot(fish_effort_2015_points, axes = FALSE, col = "red", add = TRUE) # points are large! not great for viz

# Plot the raster of fishing effort points (without making them terra points) on top of the EEZ polygons
plot(regions_eez$geometry, col = "grey96", axes = FALSE, main = "Fishing Effort 2015: AUS, USA, THA, BIH, GBR, ITA", legend = FALSE)
plot(fish_effort_2015_rast, axes = FALSE, col = "red", add = TRUE)
```

### Visualize trawling & dredging apparent fishing effort data on map of EEZ's as a time series and save the rasters as annual files.

PLot each year and save the raster as a .tif.

- would like to add checks to this, progress bar, more cores for parallelization

```{r}
tic()

#colors <- c("yellow", "red", "blue", "green", "orange", "cyan4") # 6 colors because loop does i +1, so yellow will never be plotted

years = as.factor(2012:2020) # make it factor in order to match the class of year column in the fish_effort_annual dataframe

for(i in years){
  annual_raster <- fish_effort_annual %>%
    dplyr::filter(year == i) %>%
    dplyr::select(-year) %>%
    terra::rast()

    plot(regions_eez$geometry, col = "grey96", axes = FALSE, main = paste0("Trawling and Dredging Fishing Effort in ", i, " for AUS, USA, THA, BIH, GBR, ITA", legend = FALSE))
    plot(annual_raster, axes = FALSE, col = "red", add = TRUE, legend = FALSE)
    
    # save annual raster file that encompasses trawling fishing and dredging effort for all countries
    #raster::writeRaster(annual_raster, filename = paste0(dir_M, '/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/fish_effort_', i ,'.tif'), overwrite = TRUE)
}

toc()
```

### Plot all years on same map - after all, none of the coordinates were repeated exactly on different years

```{r}
# for(i in years){
#  annual_raster <- terra::rast(paste0(dir_M, '/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/fish_effort_', i ,'.tif'))
# 
#     plot(regions_eez$geometry, col = "grey96", axes = FALSE, main = paste0("Trawing and Dredging Fishing Effort in 2012-2020 for AUS, USA, THA, BIH, GBR, ITA", legend = FALSE))
#     plot(annual_raster, axes = FALSE, col = "red", add = TRUE, legend = FALSE)
# }
# 
# toc()
```


```{r}
# fish_effort_2015 <- raster::raster(paste0(dir_M, '/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/fish_effort_2015.tif'))
# #fish_effort_2015_df <- raster::as.data.frame(fish_effort_2015, xy = TRUE, na.rm = FALSE)
# #tail(fish_effort_2015_df)
# fish_effort_2016 <- raster::raster(paste0(dir_M, '/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/fish_effort_2016.tif'))
# fish_effort_2017 <- raster::raster(paste0(dir_M, '/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/fish_effort_2017.tif'))
# fish_effort_2018 <- raster::raster(paste0(dir_M, '/git-annex/globalprep/hab_prs_hd_subtidal_soft_bottom/v2022/int/fish_effort_annual/fish_effort_2018.tif'))
```

```{r}
# must use dataframes rather than .tif files for ggplot
# ggplot() +
#   geom_raster(data = eez_df, aes(x = x, y = y, fill = 'eez')) +
#   # add eez csv for plotting:
#   geom_raster(data = fish_effort_2015_df, aes(x = x, y = y)) +
#   scale_fill_viridis_c() +
#   theme_void() +
#   theme(legend.position = "bottom") +
#   coord_equal()
```

## Plot faceted rasters for apparent fishing hours with overlaid map of EEZ's

```{r}
# eez_boundaries <- file.path()
# 
# ggplot() +
#   geom_raster(data = fish_effort_all, aes(x = lon, y = lat, fill = 'apparent_fishing_hours')) +
#   geom_sf(data = eez_boundaries, fill = NA) +
#   scale_fill_viridis_c() +
#   theme_void() +
#   theme(legend.position = "bottom") +
#   coord_equal()
```

## Create dataframe of summarized spatialized fishing effort while maintaining the administrative country for each EEZ

This step enables us to calculate scores for each region on the fishing that occurs in their EEZ?

```{r}
# recall dataframe from earlier with region variable still present: fish_effort_all
# fish_effort_regional <- fish_effort_all %>% 
#   group_by(lat, lon, year, eez_admin_rgn) %>% # only grouped by lat, lon, and year according to output, bc no 2 countries fished in the exact same coordinate in these years 
#   summarize(total_fishing_hours = sum(apparent_fishing_hours, na.rm = TRUE))
# 
# year = 2015:2018
# 
# foreach(r = rgns_eez_subset$eez_iso3) %do% {
#   foreach(yr = year) %do% {
#     fish_effort <- fish_effort_all %>%
#       dplyr::filter(year == yr) %>% 
#       dplyr::group_by(lat, lon)
#   }
# }
```

```{r}
# already created files of fishing effort separated by country, start by reading those in, group by year, sum hours
# year = 2015:2018
# foreach(r = rgns_eez_subset$eez_iso3) %do% {
#   regional_fishing_effort <- read.csv(paste0(dir_M, "/git-annex/globalprep/_raw_data/global_fishing_watch/d2022/annual_mapping_api/", r, "_effort_15_18.csv")) %>% 
#     group_by(year) %>% 
#     summarize()
  #}
```


Old Notes:
- consider open issue about needing to subset the id list in order to plug it all into get_event()?



















